\section{Introduction}
\subsection{Explanation of language model training and its relevance in natural language processing} Natural Language Processing (NLP) systems rely heavily on language models (LMs). LMs utilize computational architectures to estimate the probability distribution of a lot of various language units, such as words, phrases, sentences, and documents. Essentially, they calculate the likelihood of a particular set of words appearing in the text. \cite{KGK2016}

These models are really crucial in NLP tasks like voice recognition, machine translation, and pos tagging. To train LM, a “corpus”- a diverse collection of text datatype is necessary.  During training on this corpus, the language model internalizes patterns it discovers, allowing exact LM to produce output that might be coherent and appropriate to the given situation. Transformers are neural language models that further improve this process by learning of representing words by embedding them into the multidimensional vector spaces while fixing their semantic and syntactic characteristics. For detailed comprehension of linguistic occurrences, BERT(Bidirectional Encoder Representations from Transformers) transformer model can be used because it reads both the preceding and succeeding words in a context while understanding intricate linguistic patterns. \cite{jd2019}


However, the performance and behavior of an LM can vary really hard based on how it was trained. That is because during the training of LM, in addition to already existing learning of text patterns, LM also absorbs pre-existing within-data biases or disputed issues. While generating of a new text, trained models will usually reflect these biases, making it important to consider how corpus characteristics with any inherent contentiousness affect LM reliability.

\subsection{Statement of the research problem}
The primary focus of this study is to examine the impact of conflicting data on language models and their functionality. Initially, most of the language models are the same as giant sponges, because they absorb all information and data from the source they learn. So, what does this mean? If there is a chance that the source data contains controversial statements of the whole meaning, then consequently language models are likely to pick up and mirror these biases and controversies, which are considered as specific features of the model.

As was written before, a language model is sometimes likely to internalize and echo these biases and contradictions from income source data. In order to clarify further, consider the following scenario: for instance, there is a language model that has been trained using data sourced from social networks and media. As all of us know, such types of discussions in these spaces can be incredibly diverse and contentious, including all kinds of belittling of certain groups of people, unreliable information that cannot be qualified or verified, as well as controversial topics in which there are no rightists. Therefore, if a certain language model absorbs strong opinions or biases from that data, it can produce output text that mirrors those specific biases or controversial viewpoints. This outcome is considered as undesirable when it comes to our AI systems. 
  
So, what makes this stuff problematic? Well, let's envision a situation wherein a language model is utilized within an educational institution or a healthcare facility. If this model has been trained on controversial source data, then it may generate incorrect or even harmful outcomes – the ones we want to avoid entirely. 

Furthermore, as humanity continues integrating AI systems into various aspects of ordinary user’s daily life, trust becomes increasingly crucial. If a language model starts generating useless content or responses that users could consider as ones that don't make sense,  people may begin to doubt its reliability and lose faith in the system overall which will decrease its popularity among ordinary citizens, thus, burying the potential of the technology in general. That's why in this study, the objective is twofold. First, comprehending how conflicting data influences language models and second, discovering methods to resolve these issues in the most effective and productive way.


